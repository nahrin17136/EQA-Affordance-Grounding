# EQA-Affordance-Grounding
This repository contains a prototype of a task completion in the field of Embodied AI or Robotics. It represent a lightweight, training-free sample task for affordance-grounding and physical reasoning for manipulation in embodied question answering (EQA), inspired by ManipVQA. 

## Task
This work was completed as a take-home assignment and includes:
- Identification of a SOTA method (ManipVQA)
- Functional replication under limited compute
- Proposal of new research ideas
- Implementation of a lightweight affordance-aware prototype

## Approach
- CLIP for affordance-aware visual reasoning
- SAM for region grounding
- BLIP for physical property inference
- LLaVA for visionâ€“language grounding

## Limitations
Due to computational constraints, full end-to-end training was not performed.
The implementation serves as a proof-of-concept.

## Hardware
Experiments were conducted on Kaggle with limited GPU resources.

## Author
Nahrin Jannat


